# Product Requirements Document (PRD)

## 1. Introduction

**Project Title:** Interactive UFO/UAP Sightings Explorer & AI Analyst

This document outlines the product requirements for an interactive data application designed to explore UFO/UAP sightings. The application will be built using the Davia Python SDK and is intended to fulfill the project-based application requirement for a Data Scientist position at Davia AI. The core goal is to showcase creativity, thoughtful SDK usage, and a user-centric approach to data product development.

## 2. Goals

*   Develop a functional and engaging data application using the Davia Python SDK.
*   Demonstrate proficiency in Python for data manipulation, visualization, and API development.
*   Showcase an understanding of building user-friendly interfaces for data exploration.
*   Implement a simple AI-powered analytical assistant feature.
*   Adhere to best practices for code quality, documentation, and deployment.
*   Ensure the application can be deployed with a frontend on Vercel and a backend on Google Cloud Run, utilizing free-tier services where possible.

## 3. Target Users

*   **Primary:** Davia AI hiring team (evaluating the project for a Data Scientist role).
*   **Secondary (Implied):** Potential users of Davia-built applications (demonstrating the SDK's capability to create user-friendly tools for data exploration and analysis).

## 4. User Stories

*   **As a Davia AI evaluator, I want to** see a deployed data application so that I can assess the candidate's ability to use the Davia SDK.
*   **As a Davia AI evaluator, I want to** interact with data visualizations (maps, charts) so that I can evaluate the candidate's data presentation skills.
*   **As a Davia AI evaluator, I want to** filter data based on various criteria (date, location, UFO shape) so that I can assess the candidate's data manipulation and interactive design capabilities.
*   **As a Davia AI evaluator, I want to** see key metrics and summaries derived from the data so that I can understand the candidate's analytical abilities.
*   **As a Davia AI evaluator, I want to** interact with a simple AI assistant feature so that I can gauge the candidate's creativity and ability to integrate AI concepts.
*   **As a Davia AI evaluator, I want to** review well-structured code and documentation so that I can assess the candidate's engineering practices.
*   **As an end-user, I want to** easily explore UFO sighting data through an intuitive interface so that I can gain insights from the dataset.
*   **As an end-user, I want to** filter sightings by date, location, and shape so that I can focus on specific events.
*   **As an end-user, I want to** see sightings displayed on a map so that I can understand their geographical distribution.
*   **As an end-user, I want to** ask simple questions about the data in natural language and receive relevant answers or visualizations so that I can interact with the data more conversationally.

## 5. Functional Requirements

### 5.1. Data Loading and Processing
    *   FR1: The application must load a pre-processed dataset of UFO/UAP sightings (e.g., NUFORC dataset from Kaggle, stored in Parquet format).
    *   FR2: The backend must efficiently handle data loading and filtering operations.

### 5.2. Interactive Data Exploration & Visualization
    *   FR3: Display UFO sightings on an interactive map (e.g., scatter plot of latitude/longitude).
    *   FR4: Provide interactive filters for:
        *   Date Range (e.g., slider or date pickers).
        *   UFO Shape (e.g., dropdown menu).
        *   State/Country (e.g., dropdown menu or searchable input).
    *   FR5: Dynamically update visualizations and displayed metrics based on active filters.
    *   FR6: Display key summary metrics that update with filters, such as:
        *   Total number of sightings in the current selection.
        *   Most common UFO shape in the current selection.
        *   Month/Year with the most sightings in the current selection.
    *   FR7: (Optional, if Davia SDK supports easily) Display other relevant charts (e.g., bar chart of sightings by shape, time series of sightings).

### 5.3. AI-Powered Analytical Assistant
    *   FR8: Provide a text input for users to ask questions about the data.
    *   FR9: Implement a rule-based "AI" or a simple free-tier LLM-based system to interpret user queries and respond with:
        *   Direct textual answers (e.g., "How many sightings in California?" -> "There were X sightings in California.").
        *   Updates to the data visualizations/filters (e.g., "Show me triangle shapes in Texas").
    *   FR10: The AI assistant should handle a predefined set of common questions relevant to the dataset.

### 5.4. User Interface (via Davia SDK)
    *   FR11: The application must have a clean, intuitive, and user-friendly interface generated by the Davia SDK.
    *   FR12: All interactive elements (filters, map, charts, AI input) must be clearly presented and easy to use.

## 6. Non-Functional Requirements

### 6.1. Performance
    *   NFR1: Data filtering and visualization updates should feel responsive (ideally within 1-2 seconds for most operations on the pre-processed dataset).
    *   NFR2: The initial application load time should be reasonable.

### 6.2. Scalability
    *   NFR3: The backend, deployed on Google Cloud Run, should be ableto handle a small number of concurrent users (sufficient for evaluation purposes).
    *   NFR4: The application design should consider efficient data handling for the given dataset size.

### 6.3. Usability
    *   NFR5: The application must be easy to understand and navigate without requiring external instructions for basic exploration.

### 6.4. Maintainability
    *   NFR6: The Python code for the backend (API) must be well-structured, commented, and follow Python best practices.
    *   NFR7: Configuration files (e.g., `vercel.json`) should be clear and well-organized.

### 6.5. Deployability
    *   NFR8: The frontend must be deployable on Vercel.
    *   NFR9: The Python backend (API) must be deployable on Google Cloud Run.
    *   NFR10: Deployment scripts and configurations should be provided or clearly documented.

### 6.6. Cost-Effectiveness
    *   NFR11: The project must primarily use free-tier services for dataset storage, hosting (Vercel, Google Cloud Run), and any AI capabilities.

### 6.7. Documentation
    *   NFR12: Comprehensive project documentation must be provided, covering product requirements, technical architecture, API standards, deployment, etc. (as outlined in `00b-Master-Documentation-Index.md`).

## 7. API (OpenAPI Specification)
    *   The backend API will expose endpoints for data retrieval, filtering, and AI query processing.
    *   The API will be defined using an OpenAPI 3.0 specification, as this is what Davia uses to build the frontend.
    *   Details of the API will be in `04-API-Documentation-Standards.md`.

## 8. Assumptions and Dependencies
    *   The Davia Python SDK provides the necessary components to build the required interactive UI elements (filters, charts, text inputs).
    *   A suitable, free, and publicly available UFO/UAP sightings dataset (e.g., NUFORC from Kaggle) can be obtained and pre-processed.
    *   Vercel and Google Cloud Run free tiers are sufficient for hosting and evaluating the application.
    *   The "AI assistant" can be implemented effectively using a rule-based approach or a free-tier LLM if available and simple to integrate.

## 9. Success Criteria
    *   A fully functional data application is deployed and accessible via a public URL.
    *   All specified functional requirements (data loading, filtering, visualization, AI assistant basics) are met.
    *   The application is user-friendly and provides a good interactive experience.
    *   The project demonstrates a clear understanding and thoughtful use of the Davia SDK.
    *   The codebase is well-organized, and the documentation is complete and clear.
    *   The project successfully conveys the candidate's skills in Python, data science, and product thinking.
